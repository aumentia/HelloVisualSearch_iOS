{"name":"Visual Search Framework [iOS]","tagline":"Aumentia Visual Search Framework: Image Recognition and QR / bar code reader","body":"### The Visual Search Framework includes:\r\n![VS Framework Logo](https://s3-us-west-1.amazonaws.com/aumentia/wiki/home/icon55.png)\r\n![Recognise within milliseconds any image](https://s3-us-west-1.amazonaws.com/aumentia/wiki/home/visualsearch_small.png)\r\n* Add visual search to your app.\r\n* Real time image matching (50-100 images per pool).\r\n* Insert images from local resources: works offline!\r\n* Insert images from URL.\r\n* White Label.\r\n* QR code scan support.\r\n* Scan up to 4 QR / bar codes at a time.\r\n* Define you matching areas: Regions of Interest.\r\n* Match images and QR codes simultaneously \r\n* **arm64 support**\r\n* **Swift support**\r\n\r\n## NEW: Motion Recognition\r\n* Define virtual buttons (regions of interest) to detect motion\r\n\r\n## Content\r\n*  [Configure VS Framework](https://github.com/aumentia/HelloVisualSearch_iOS/wiki/Configure-VS-Framework)\r\n*  [Add & Remove Images](https://github.com/aumentia/HelloVisualSearch_iOS/wiki/Add-&-Remove-Images)\r\n*  [Read QR & Bar codes](https://github.com/aumentia/HelloVisualSearch_iOS/wiki/Read-QR---bar-codes)\r\n*  [Crop frames](https://github.com/aumentia/HelloVisualSearch_iOS/wiki/Crop-frames)\r\n*  [Motion Detection (Virtual Buttons)](https://github.com/aumentia/HelloVisualSearch_iOS/wiki/Motion-Detection-(-Virtual-Buttons-))\r\n*  [Connect to SWIFT](https://github.com/aumentia/HelloVisualSearch_iOS/wiki/Connect-to-SWIFT)\r\n*  [API](http://api.aumentia.com/visualsearch/)\r\n\r\n## Configure the framework\r\n\r\n* Include the following frameworks\r\n\r\n![Include these frameworks](https://s3-us-west-1.amazonaws.com/aumentia/wiki/configure/vs1.png)\r\n\r\n\r\n* Add the below linker flags:\r\n`-mthumb -lstdc++ -lz -lm -mfpu=neon -mtune=cortex-a8` \r\n\r\n![Other Linker Flags](https://s3-us-west-1.amazonaws.com/aumentia/wiki/configure/vs2.png)\r\n\r\n\r\n* Include the **VS.framework** and add its protocols \r\n```objective-c\r\n#import <VS/vsPlugin.h>\r\n...\r\n \r\n@interface ViewController : UIViewController<imageMatchedProtocol, QRMatchedProtocol,...>\r\n\r\n@end\r\n```\r\n\r\n* The .m file where you include the framework must be compiled supporting cpp, so change the “**File Type**” to “**Objective-C++ Source**“. \r\n\r\n![objc++](https://s3-us-west-1.amazonaws.com/aumentia/wiki/configure/vs3.png)\r\n\r\n* Add your own camera. You should create a protocol that will be receiving each output frame. We will send this frame to the engine to analyse it.\r\n We provide the CaptureSessionManager class to take care of all the camera aspects.\r\n You just will need to set it up:\r\n\r\n```objective-c\r\n#pragma mark - Add external camera\r\n \r\n- (void)initCapture {\r\n     \r\n    // init capture manager\r\n    _captureManager = [[CaptureSessionManager alloc] init];\r\n     \r\n    _captureManager.delegate = self;\r\n     \r\n    // set video streaming quality\r\n    _captureManager.captureSession.sessionPreset = AVCaptureSessionPresetMedium;   //480x360\r\n     \r\n    [_captureManager setOutPutSetting:[NSNumber numberWithInt:kCVPixelFormatType_32BGRA]]; //kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange\r\n     \r\n    [_captureManager addVideoInput:AVCaptureDevicePositionBack]; //AVCaptureDevicePositionFront / AVCaptureDevicePositionBack\r\n    [_captureManager addVideoOutput];\r\n    [_captureManager addVideoPreviewLayer];\r\n     \r\n    CGRect layerRect = self.view.bounds;\r\n     \r\n    [[_captureManager previewLayer] setOpaque: 0];\r\n    [[_captureManager previewLayer] setBounds:layerRect ];\r\n    [[_captureManager previewLayer] setPosition:CGPointMake( CGRectGetMidX(layerRect), CGRectGetMidY(layerRect) ) ];\r\n     \r\n    // create a view, on which we attach the AV Preview layer\r\n    _cameraView = [[UIView alloc] initWithFrame:self.view.bounds];\r\n    [[_cameraView layer] addSublayer:[_captureManager previewLayer]];\r\n     \r\n    // add the view we just created as a subview to the View Controller's view\r\n    [self.view addSubview: _cameraView];\r\n     \r\n    // start !\r\n    [self performSelectorInBackground:@selector(start_captureManager) withObject:nil];\r\n     \r\n}\r\n \r\n- (void)removeCapture\r\n{\r\n    [_captureManager.captureSession stopRunning];\r\n    [_cameraView removeFromSuperview];\r\n    _captureManager     = nil;\r\n    _cameraView         = nil;\r\n}\r\n \r\n- (void)start_captureManager\r\n{\r\n    @autoreleasepool\r\n    {\r\n        [[_captureManager captureSession] startRunning];\r\n    }\r\n}\r\n```\r\nand implement its callbacks:\r\n\r\n```objective-c\r\n#pragma mark - External Camera Delegates\r\n \r\n- (void)processNewCameraFrameRGB:(CVImageBufferRef)cameraFrame\r\n{\r\n    [_myVs processRGBFrame:cameraFrame saveImageToPhotoAlbum:NO ];\r\n}\r\n \r\n- (void)processNewCameraFrameYUV:(CVImageBufferRef)cameraFrame\r\n{\r\n    [_myVs processYUVFrame:cameraFrame saveImageToPhotoAlbum:NO];\r\n}\r\n```\r\n\r\n## Add & Remove Images\r\n\r\nThe library matches the camera output frames with the images stored in its database. The search process takes few ms, this means it is real time. The processing time depends on the device and on the amount of images you add to the system. Typically you can add around **50 images up to 100** in the most powerful devices.\r\nYou can both add images from **URL** or from your **local** resources.\r\nYou can also set up your unique Id or let the library do it for you.\r\n\r\nTo add images you can use these functions:\r\n\r\n```objective-c\r\n- (NSInteger) insertImage:(UIImage*)image;\r\n \r\n- (NSInteger) insertImageFromURL:(NSURL*)imageUrl;\r\n \r\n- (BOOL) insertImage:(UIImage*)image withId:(NSInteger)uId;\r\n \r\n- (BOOL) insertImageFromURL:(NSURL*)imageUrl withId:(NSInteger)uId;\r\n```\r\n\r\nYou can also delete a single image providing its unique id:\r\n\r\n```objective-c\r\n- (BOOL) deleteImage:(NSInteger)uId;\r\n```\r\n\r\nOr delete all the images in the db.\r\n\r\n```objective-c\r\n- (BOOL) deleteAllImages;\r\n```\r\n\r\n### How to create a “good pool”\r\n\r\nA good pool is a group of images that have good features, are different between them and can be easily matched without getting any false positive. In order to assure that you create good pools we provide several functions:\r\n\r\n* **setMatchingThreshold**: The images added to the system receive a punctuation between 0 and 10 indicating how “rich” is the image (amounf of good features for matching). The more close to 10, easier is to match the image and more difficult is to get a false positive.\r\n    Setting this param, the user decides the minimum quality of his pool of images.\r\n    \r\n* Before adding an image to your pool you can get its punctuation with the **getImageScore** function.\r\n \r\n* You can also adjust the **MinFeatures** and **MaxFeatures** params:\r\n\r\n    **MinFeatures**: Minimum amount of features accepted to insert an image in the system.\r\n    * The higher it is the most feature-rich images will be required to be inserted.\r\n    * This value is related with the threshold: the higher the most restrictive the threshold will be.\r\n    * **Default value is 50.**\r\n\r\n    **MaxFeatures**: Maximum amount of features accepted to match an image.\r\n    * When an image / frame is going to be matched against the images already inserted in the system, the engine\r\n    * extract the best features and compares them with the ones of the images in the system.\r\n    * The higher it is the most features will be accepted in the matching process.\r\n    * The higher it is the better will be the matching quality but the matching speed will be reduced.\r\n    * **Default value is 50.**\r\n    \r\n* It is recommended to use the motion filter. This filter checks if the device is moving and if so, it does not look for matches avoiding false positive with undefined frames while moving: \r\n\r\n```objective-c\r\n- (void)initMotionDetection;\r\n \r\n- (void)removeMotionDetection;\r\n```\r\n\r\n### Detection callbacks\r\n\r\nFirst you need to implement the **CaptureSessionManger** class callbacks. Remember that those callbacks will provide you the video feed frames, frames that you will send to be analysed by the library.\r\n\r\n```objective-c\r\n- (void)processNewCameraFrameRGB:(CVImageBufferRef)cameraFrame\r\n{\r\n    [_myVs processRGBFrame:cameraFrame saveImageToPhotoAlbum:NO ];\r\n}\r\n```\r\n\r\n```objective-c \r\n- (void)processNewCameraFrameYUV:(CVImageBufferRef)cameraFrame\r\n{\r\n    [_myVs processYUVFrame:cameraFrame saveImageToPhotoAlbum:NO];\r\n}\r\n```\r\n\r\nThen the engine will give you back the matching results: -1 if no image is recognised, != -1 if an image has been matched, being that result its unique id.\r\n\r\n```objective-c \r\n- (void)imageMatchedResult:(NSInteger)uId\r\n{\r\n    if (uId != -1)\r\n    {\r\n        VSLog(@\"Image detected --> %d\", uId);\r\n    }\r\n}\r\n```\r\n\r\n## QR / bar codes\r\n\r\nThis library is capable also to read QR and bar codes.\r\nThe supported standards are:\r\n*     UPC-A\r\n*     UPC-E\r\n*     EAN-8\r\n*     EAN-13\r\n*     Code 39\r\n*     Code 93\r\n*     Code 128\r\n*     ITF\r\n*     Codabar\r\n*     RSS-14 (all variants)\r\n*     QR Code\r\n*     Data Matrix\r\n*     Aztec (‘beta’ quality)\r\n*     PDF 417 (‘alpha’ quality)\r\n\r\nIn order to enable this feature, set **search_mode** to **search_QRcodes** or **search_all**\r\n\r\n### Multiple QR / bar codes\r\n\r\nYou can also read **multiple QR / bar codes a time**. To do that you will need to define ROIs, regions of interest, where to “look for” readable codes.\r\n\r\n```objective-c\r\n- (void)addQRRois\r\n{\r\n    // Add regions to match several QR / bar codes\r\n    Roi *ROI1 = [[Roi alloc] initWithRect:CGRectMake(0, 0, 160, 240)];\r\n    Roi *ROI2 = [[Roi alloc] initWithRect:CGRectMake(0, 240, 160, 240)];\r\n    Roi *ROI3 = [[Roi alloc] initWithRect:CGRectMake(160, 0, 160, 240)];\r\n    Roi *ROI4 = [[Roi alloc] initWithRect:CGRectMake(160, 240, 160, 240)];\r\n        \r\n    // Add them to the system\r\n    [_myVs addQRRect:ROI1];\r\n    [_myVs addQRRect:ROI2];\r\n    [_myVs addQRRect:ROI3];\r\n    [_myVs addQRRect:ROI4];\r\n}\r\n```\r\n\r\n### QR codes callbacks\r\n\r\nWhen a single or multiple QR / bar code is/are read, the QRMatchedProtocol protocol will give you the value/s:\r\n\r\n```objective-c\r\n\r\n- (void)singleQRMatchedResult:(NSString *)res\r\n{\r\n    if ( ![res isEqualToString:@\"\"])\r\n    {\r\n        VSLog(@\"QR / bar code detected --> %@\", res);\r\n    }\r\n}\r\n \r\n- (void)multipleQRMatchedResult:(NSArray *)codes\r\n{\r\n    NSString *code = @\"\";\r\n     \r\n    for (int i = 0; i < [codes count]; i++)\r\n    {\r\n        Roi *roi = [codes objectAtIndex:i];\r\n         \r\n        code = 1;\r\n    }\r\n     \r\n    VSLog(@\"Multiple QR / bar codes detected --> %@\", code);\r\n}\r\n```\r\n\r\n## Crop frames\r\n\r\nIf you don't want to analyse the whole frame but part of it, you can crop it:\r\n\r\n```objective-c\r\n- (void)addCropRect\r\n{\r\n    CGRect myRect = CGRectMake(20, 20, 200, 125);\r\n     \r\n    [self addRectToView:myRect];\r\n     \r\n    [_myVs setImageCropRect:myRect];\r\n}\r\n```\r\n\r\n## Motion Detection\r\n\r\nCreate **virtual buttons** by detecting motion within regions of interest that you define.\r\n\r\n### Init a motion detection instance\r\n\r\nImport the VSMotion header:\r\n\r\n```objective-c\r\n#import <VS/VSMotion.h>\r\n```\r\n\r\nInit the library setting your **API Key** and the **DEBUG** flag. If that FLAG is set to **YES**, you will get on the console some useful debug messages.\r\n\r\n```objective-c\r\nVSMotion *aumMotion = [[VSMotion alloc] initWithKey:@\"749c6205e9c035d3850b509aa94d1bb5d8d89b4a\" setDebug:YES];\r\n```\r\n\r\n### Configure the VSMotion\r\n\r\nEnable the **motion detection filter**: the engine will not search if the device is moving. This option makes it more difficult to get false positives. The threshold goes from 0 to 10 being 0 very sensitive and 10 less sensitive.\r\n\r\n```objective-c\r\n[aumMotion initMotionDetectionWithThreshold:3 enableDebugLog:NO];\r\n```\r\n\r\nSet the number of frames the virtual buttons will remain inactive after one is triggered.\r\n\r\n```objective-c\r\n[aumMotion setInactivePeriod:[NSNumber numberWithInt:LOWDELAY]];\r\n```\r\n\r\n### Add and Remove ROIs ( regions of interest ) or virtual buttons\r\n\r\nTo add a ROI just define a CGRect taking in consideration that the values are represented in percentages ( values from 0 to 100 ). E.g. CGRect(10, 10, 25, 50) x = 0.1*screen_width, y = 0.1*screen_height, w = 0.25*screen_width and h = 0.50 *screen_height\r\n\r\n```objective-c\r\nCGRect ROI1 = CGRectMake(5, 5, 20, 20);\r\n[aumMotion addButtonWithRect:ROI1];\r\n```\r\n\r\nYou can remove the ROIs one by one with their unique Id or clear all of them at once:\r\n\r\n```objective-c\r\n[aumMotion clearButtons];\r\n```\r\n\r\n### Callbacks\r\n\r\nYour class should implement the **VSMotionProtocol**\r\n\r\nheader:\r\n```objective-c\r\n@interface ViewController : UIViewController<VSMotionProtocol, CameraCaptureDelegate>\r\n```\r\n\r\nimplementation:\r\n```objective-c\r\n[aumMotion setVSMotionDelegate:self];\r\n```\r\n\r\nWe got 2 functions: **buttonClicked** and **buttonsActive**\r\n\r\nThe first one will be called when a button is clicked providing the button unique id.\r\n\r\n```objective-c\r\n - (void)buttonClicked:(NSNumber *)buttonId\r\n{\r\n    VSLog(\"Clicked button %d\", buttonId.intValue);\r\n}\r\n```\r\n\r\nWhen a button is clicked all the rest remain inactive for some frames. The second function will let us know if the buttons are active or inactive.\r\n\r\n```objective-c\r\n- (void)buttonsActive:(BOOL)isActive\r\n{\r\n    if ( !isActive )\r\n    {\r\n        VSLog(\"Buttons disabled\");\r\n    }\r\n}\r\n```\r\n\r\n## Connect to SWIFT\r\n\r\nThe Visual Search Framework is written in Objective C. In order to connect it to your Swift app follow these steps:\r\n\r\n* **Import the VS.framework** by dragging and dropping it into an Xcode 6 Swift project.\r\n\r\n* **Create a new Objective C file** in your project (File->New->File [Objective C for iOS]).\r\n\r\n* **Accept the prompt** to create a **bridging header** file between Objective C and Swift.\r\n\r\n* **Delete the newly created Objective C** file but retain the bridging header file ${YOURPROJ}-Bridging-Header.h.\r\n\r\n* In the Bridging header file, import the VS.framework using the standard Objective C import syntax. You will need to import as well all the dependencies of the framework.\r\n\r\n```objective-c\r\n#import <Foundation/Foundation.h>\r\n#import <UIKit/UIKit.h>\r\n#import <CoreVideo/CoreVideo.h>\r\n#import <CoreGraphics/CoreGraphics.h>\r\n#import <QuartzCore/QuartzCore.h>\r\n#import <Accelerate/Accelerate.h>\r\n#import <CoreMotion/CoreMotion.h>\r\n#import <AVFoundation/AVFoundation.h>\r\n\r\n#import <VS/VS.h>\r\n```\r\n\r\n* Set Build Settings -> Packaging -> **Defines Module** to **Yes**\r\n\r\n* Add Build Phases -> Link Binary With Libraries -> **libstdc++.6.0.9.dylib** ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}